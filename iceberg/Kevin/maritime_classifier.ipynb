{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (1122, 75, 75, 2) (1122, 2)\n",
      "Validation (482, 75, 75, 2) (482, 2)\n",
      "Train (1122, 75, 75, 2) (1122, 2)\n",
      "Validation (482, 75, 75, 2) (482, 2)\n",
      "Adding CONV Layers\n",
      "Adding Fully-Connected Layer\n",
      "Adding Output Layer\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_1 (Batch (None, 75, 75, 2)         8         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 73, 73, 16)        304       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 34, 34, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 17, 17, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 15, 15, 48)        13872     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 48)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2352)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               235300    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 254,326\n",
      "Trainable params: 254,322\n",
      "Non-trainable params: 4\n",
      "_________________________________________________________________\n",
      "Training\n",
      "Train on 1122 samples, validate on 482 samples\n",
      "Epoch 1/40\n",
      "1122/1122 [==============================] - 1s - loss: 2.6505 - acc: 0.6381 - val_loss: 2.1869 - val_acc: 0.5954\n",
      "Epoch 2/40\n",
      "1122/1122 [==============================] - 0s - loss: 1.6965 - acc: 0.7718 - val_loss: 2.3389 - val_acc: 0.4938\n",
      "Epoch 3/40\n",
      "1122/1122 [==============================] - 0s - loss: 1.2951 - acc: 0.8004 - val_loss: 1.3945 - val_acc: 0.6929\n",
      "Epoch 4/40\n",
      "1122/1122 [==============================] - 0s - loss: 1.0897 - acc: 0.8253 - val_loss: 1.3675 - val_acc: 0.6245\n",
      "Epoch 5/40\n",
      "1122/1122 [==============================] - 0s - loss: 0.9499 - acc: 0.8458 - val_loss: 1.2536 - val_acc: 0.6701\n",
      "Epoch 6/40\n",
      "1122/1122 [==============================] - 0s - loss: 0.8829 - acc: 0.8556 - val_loss: 0.9365 - val_acc: 0.8278\n",
      "Epoch 7/40\n",
      "1122/1122 [==============================] - 0s - loss: 0.8153 - acc: 0.8601 - val_loss: 0.9990 - val_acc: 0.7531\n",
      "Epoch 8/40\n",
      "1122/1122 [==============================] - 0s - loss: 0.7534 - acc: 0.8672 - val_loss: 0.9153 - val_acc: 0.7365\n",
      "Epoch 9/40\n",
      "1122/1122 [==============================] - 0s - loss: 0.6825 - acc: 0.8957 - val_loss: 0.8220 - val_acc: 0.7988\n",
      "Epoch 10/40\n",
      "1122/1122 [==============================] - 0s - loss: 0.6472 - acc: 0.8993 - val_loss: 0.7766 - val_acc: 0.8154\n",
      "Epoch 11/40\n",
      "1122/1122 [==============================] - 0s - loss: 0.6210 - acc: 0.8939 - val_loss: 0.7681 - val_acc: 0.8050\n",
      "Epoch 12/40\n",
      "1122/1122 [==============================] - 0s - loss: 0.6040 - acc: 0.8824 - val_loss: 0.6567 - val_acc: 0.8402\n",
      "Epoch 13/40\n",
      "1122/1122 [==============================] - 0s - loss: 0.5454 - acc: 0.9162 - val_loss: 0.6300 - val_acc: 0.8506\n",
      "Epoch 14/40\n",
      "1122/1122 [==============================] - 0s - loss: 0.5185 - acc: 0.9216 - val_loss: 0.6226 - val_acc: 0.8527\n",
      "Epoch 15/40\n",
      "1122/1122 [==============================] - 0s - loss: 0.5114 - acc: 0.9207 - val_loss: 0.5807 - val_acc: 0.8548\n",
      "Epoch 16/40\n",
      "1122/1122 [==============================] - 0s - loss: 0.4976 - acc: 0.9118 - val_loss: 0.6314 - val_acc: 0.8299\n",
      "Epoch 17/40\n",
      "1122/1122 [==============================] - 0s - loss: 0.4862 - acc: 0.9144 - val_loss: 0.5819 - val_acc: 0.8610\n",
      "Epoch 18/40\n",
      "1122/1122 [==============================] - 0s - loss: 0.4608 - acc: 0.9144 - val_loss: 0.5737 - val_acc: 0.8548\n",
      "Epoch 19/40\n",
      "1122/1122 [==============================] - 0s - loss: 0.4418 - acc: 0.9332 - val_loss: 0.5508 - val_acc: 0.8527\n",
      "Epoch 20/40\n",
      "1122/1122 [==============================] - 0s - loss: 0.4417 - acc: 0.9305 - val_loss: 0.5330 - val_acc: 0.8755\n",
      "Epoch 21/40\n",
      "1122/1122 [==============================] - 0s - loss: 0.4406 - acc: 0.9278 - val_loss: 0.6056 - val_acc: 0.8423\n",
      "Epoch 22/40\n",
      "1122/1122 [==============================] - 0s - loss: 0.4256 - acc: 0.9323 - val_loss: 0.5222 - val_acc: 0.8631\n",
      "Epoch 23/40\n",
      "1122/1122 [==============================] - 0s - loss: 0.4011 - acc: 0.9385 - val_loss: 0.6399 - val_acc: 0.8195\n",
      "Epoch 24/40\n",
      "1122/1122 [==============================] - 0s - loss: 0.3991 - acc: 0.9421 - val_loss: 0.5633 - val_acc: 0.8485\n",
      "Epoch 25/40\n",
      "1122/1122 [==============================] - 0s - loss: 0.3860 - acc: 0.9332 - val_loss: 0.5056 - val_acc: 0.8838\n",
      "Epoch 26/40\n",
      "1122/1122 [==============================] - 0s - loss: 0.3615 - acc: 0.9501 - val_loss: 0.6692 - val_acc: 0.8154\n",
      "Epoch 27/40\n",
      "1122/1122 [==============================] - 0s - loss: 0.3849 - acc: 0.9332 - val_loss: 0.5094 - val_acc: 0.8776\n",
      "Epoch 28/40\n",
      "1122/1122 [==============================] - 0s - loss: 0.3672 - acc: 0.9358 - val_loss: 0.4924 - val_acc: 0.8797\n",
      "Epoch 29/40\n",
      "1122/1122 [==============================] - 0s - loss: 0.3613 - acc: 0.9394 - val_loss: 0.4814 - val_acc: 0.8963\n",
      "Epoch 30/40\n",
      "1122/1122 [==============================] - 0s - loss: 0.3324 - acc: 0.9545 - val_loss: 0.4861 - val_acc: 0.8900\n",
      "Epoch 31/40\n",
      "1122/1122 [==============================] - 0s - loss: 0.3574 - acc: 0.9474 - val_loss: 0.5953 - val_acc: 0.8257\n",
      "Epoch 32/40\n",
      "1122/1122 [==============================] - 0s - loss: 0.3409 - acc: 0.9439 - val_loss: 0.5179 - val_acc: 0.8631\n",
      "Epoch 33/40\n",
      "1122/1122 [==============================] - 0s - loss: 0.3042 - acc: 0.9652 - val_loss: 0.4946 - val_acc: 0.8838\n",
      "Epoch 34/40\n",
      "1122/1122 [==============================] - 0s - loss: 0.3298 - acc: 0.9474 - val_loss: 0.4831 - val_acc: 0.8693\n",
      "Epoch 35/40\n",
      "1122/1122 [==============================] - 0s - loss: 0.3299 - acc: 0.9545 - val_loss: 0.4865 - val_acc: 0.8672\n",
      "Epoch 36/40\n",
      "1122/1122 [==============================] - 0s - loss: 0.3297 - acc: 0.9537 - val_loss: 0.4752 - val_acc: 0.8859\n",
      "Epoch 37/40\n",
      "1122/1122 [==============================] - 0s - loss: 0.3198 - acc: 0.9572 - val_loss: 0.4986 - val_acc: 0.8797\n",
      "Epoch 38/40\n",
      "1122/1122 [==============================] - 0s - loss: 0.3050 - acc: 0.9599 - val_loss: 0.5139 - val_acc: 0.8631\n",
      "Epoch 39/40\n",
      "1122/1122 [==============================] - 0s - loss: 0.2864 - acc: 0.9652 - val_loss: 0.5098 - val_acc: 0.8714\n",
      "Epoch 40/40\n",
      "1122/1122 [==============================] - 0s - loss: 0.2745 - acc: 0.9750 - val_loss: 0.4717 - val_acc: 0.8838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"predicting\")\\ntest_predictions = cnn.predict(test_images)\\n\\npred_df = test_df[[\\'id\\']].copy()\\npred_df[\\'is_iceberg\\'] = test_predictions[:,1]\\nprint(\"creating csv\")\\npred_df.to_csv(\\'predictions.csv\\', index = False) '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import os.path as path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.util.montage import montage2d\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, BatchNormalization, Dropout, MaxPooling2D, Dense, Flatten, ZeroPadding2D, Activation\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.regularizers import l2\n",
    "from extra_functions import *\n",
    "\n",
    "\n",
    "def load_and_format(in_path):\n",
    "    out_df = pd.read_json(in_path)\n",
    "    out_images = out_df.apply(lambda c_row: [np.stack([c_row['band_1'],c_row['band_2']], -1).reshape((75,75,2))],1)\n",
    "    out_images = np.stack(out_images).squeeze()\n",
    "    return out_df, out_images\n",
    "\n",
    "dir_path = path.abspath(path.join('__file__',\"../..\"))\n",
    "train_path = dir_path + \"/train.json\"\n",
    "test_path = dir_path + \"/test.json\"\n",
    "train_df, train_images = load_and_format(train_path)\n",
    "test_df, test_images = load_and_format(test_path)\n",
    "\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_images,\n",
    "                                                   to_categorical(train_df['is_iceberg']),\n",
    "                                                    random_state = 2017,\n",
    "                                                    test_size = 0.3\n",
    "                                                   )\n",
    "print('Train', x_train.shape, y_train.shape)\n",
    "print('Validation', x_val.shape, y_val.shape)\n",
    "\n",
    "\"\"\"\n",
    "print(\"reading data\")\n",
    "x_train = pd.read_csv('x_train.csv', header=None)\n",
    "x_train = x_train.values\n",
    "y_train = pd.read_csv('y_train.csv', header=None)\n",
    "y_train = y_train.values\n",
    "x_val = pd.read_csv('x_val.csv', header=None)\n",
    "x_val = x_val.values\n",
    "y_val = pd.read_csv('y_val.csv', header=None)\n",
    "y_val = y_val.values  \n",
    "\n",
    "print(\"reshaping data\")\n",
    "x_train = np.reshape(x_train, (3366, 75, 75, 2))\n",
    "x_val = np.reshape(x_val, (482, 75, 75, 2)) \"\"\"\n",
    "\n",
    "print('Train', x_train.shape, y_train.shape)\n",
    "print('Validation', x_val.shape, y_val.shape) \n",
    "\n",
    "cnn = Sequential()\n",
    "#Original: 0.0005\n",
    "#0.001 -- Pretty good, better than original\n",
    "#0.01 -- Pretty good, less consistent than above but higher peak\n",
    "#0.1 -- Very good, peak of 0.8859, quick rise as well, decently consistent\n",
    "#0.5 -- Very good, 0.89 at epoch 127, peak of 0.8963 at epoch 225, slower to rise, ok consistent\n",
    "#0.25 -- Very good, 100 epochs has 0.9004, ok consistent\n",
    "#0.25 w/128 batch -- Very good, 95 epochs has 0.9004, more consistent stretches, slower to rise\n",
    "\n",
    "#0.01 -- pretty good -- 0.88, pretty consistent\n",
    "#0.1 -- pretty good -- 0.9, ok consistent\n",
    "\n",
    "weight_decay = 0.01\n",
    "\n",
    "#Preprocess Data with Mean Normalization\n",
    "cnn.add(BatchNormalization(input_shape=(75, 75, 2)))\n",
    "\n",
    "#Adding Layers\n",
    "print(\"Adding CONV Layers\")\n",
    "cnn.add(Conv2D(16, kernel_size=(3,3), activation='relu', kernel_regularizer=l2(weight_decay)))\n",
    "#cnn.add(BatchNormalization())\n",
    "\n",
    "\n",
    "#cnn.add(Conv2D(16, kernel_size=(3,3), padding = 'same', activation='relu'))\n",
    "#cnn.add(BatchNormalization())\n",
    "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "#cnn.add(Conv2D(32, kernel_size=(3,3), padding = 'same', activation='relu'))\n",
    "#cnn.add(BatchNormalization())\n",
    "#cnn.add(Dropout(0.3))\n",
    "\n",
    "cnn.add(Conv2D(32, kernel_size=(3,3), activation='relu', kernel_regularizer=l2(weight_decay)))\n",
    "#cnn.add(BatchNormalization())\n",
    "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "cnn.add(Conv2D(48, kernel_size=(3,3), activation='relu', kernel_regularizer=l2(weight_decay)))\n",
    "#cnn.add(BatchNormalization())\n",
    "\n",
    "#cnn.add(Conv2D(48, kernel_size=(3,3), padding = 'same', activation='relu'))\n",
    "#cnn.add(BatchNormalization())\n",
    "\n",
    "#cnn.add(Conv2D(48, kernel_size=(3,3), padding = 'same', activation='relu', kernel_regularizer=l2(weight_decay)))\n",
    "#cnn.add(BatchNormalization())\n",
    "#cnn.add(Dropout(0.3))\n",
    "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "#Add Fully-Connected Layer\n",
    "print(\"Adding Fully-Connected Layer\")\n",
    "#Flatten so that the data can pass through a FC Layer\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(100, activation='relu', kernel_regularizer=l2(weight_decay)))\n",
    "#cnn.add(BatchNormalization())\n",
    "cnn.add(Dropout(0.3))\n",
    "\n",
    "#Add Output Layer\n",
    "print(\"Adding Output Layer\")\n",
    "cnn.add(Dense(2, activation='softmax'))\n",
    "\n",
    "#Define loss function and optimizer\n",
    "cnn.compile(optimizer='adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "cnn.summary()\n",
    "print(\"Training\")\n",
    "cnn.fit(x_train, y_train, batch_size = 64, validation_data = (x_val, y_val), epochs = 100, shuffle = True)\n",
    "\n",
    "\"\"\"\n",
    "print(\"predicting\")\n",
    "test_predictions = cnn.predict(test_images)\n",
    "\n",
    "pred_df = test_df[['id']].copy()\n",
    "pred_df['is_iceberg'] = test_predictions[:,1]\n",
    "print(\"creating csv\")\n",
    "pred_df.to_csv('predictions.csv', index = False) \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
