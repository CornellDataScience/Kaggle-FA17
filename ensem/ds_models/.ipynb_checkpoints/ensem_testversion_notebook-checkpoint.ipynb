{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MLENS] backend: threading\n",
      "/home/joseph/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/joseph/kaggle/zillow/train_2016_v2.csv\n",
      "We exclude: ['architecturalstyletypeid', 'basementsqft', 'buildingclasstypeid', 'decktypeid', 'finishedsquarefeet13', 'finishedsquarefeet6', 'poolsizesum', 'pooltypeid10', 'pooltypeid2', 'storytypeid', 'typeconstructiontypeid', 'yardbuildingsqft26', 'fireplaceflag', 'taxdelinquencyflag', 'taxdelinquencyyear']\n",
      "15\n",
      "We exclude: ['transaction_year', 'buildingclasstypeid', 'decktypeid', 'hashottuborspa', 'poolcnt', 'pooltypeid10', 'pooltypeid2', 'pooltypeid7', 'storytypeid', 'fireplaceflag', 'assessmentyear', 'taxdelinquencyflag']\n",
      "12\n",
      "We use these for training: ['transaction_month', 'transaction_day', 'transaction_quarter', 'airconditioningtypeid', 'bathroomcnt', 'bedroomcnt', 'buildingqualitytypeid', 'calculatedbathnbr', 'finishedfloor1squarefeet', 'calculatedfinishedsquarefeet', 'finishedsquarefeet12', 'finishedsquarefeet15', 'finishedsquarefeet50', 'fips', 'fireplacecnt', 'fullbathcnt', 'garagecarcnt', 'garagetotalsqft', 'heatingorsystemtypeid', 'latitude', 'longitude', 'lotsizesquarefeet', 'propertycountylandusecode', 'propertylandusetypeid', 'rawcensustractandblock', 'regionidcity', 'regionidcounty', 'regionidneighborhood', 'regionidzip', 'roomcnt', 'threequarterbathnbr', 'unitcnt', 'yardbuildingsqft17', 'yearbuilt', 'numberofstories', 'structuretaxvaluedollarcnt', 'taxvaluedollarcnt', 'landtaxvaluedollarcnt', 'taxamount', 'censustractandblock']\n",
      "40\n",
      "Cat features are: ['transaction_month', 'transaction_day', 'transaction_quarter', 'airconditioningtypeid', 'buildingqualitytypeid', 'fips', 'heatingorsystemtypeid', 'propertycountylandusecode', 'propertylandusetypeid', 'regionidcity', 'regionidcounty', 'regionidneighborhood', 'regionidzip', 'yearbuilt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joseph/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:237: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/joseph/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:246: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding baseline models to ensembler\n",
      "training ensembler\n",
      "predicting on lightgbm\n",
      "predicting on lightgbm\n",
      "predicting on lightgbm\n",
      "predicting on lightgbm\n",
      "predicting on lightgbm\n",
      "predicting on xgboost\n",
      "predicting on xgboost\n",
      "predicting on xgboost\n",
      "predicting on xgboost\n",
      "predicting on xgboost\n",
      "predicting on lightgbm\n",
      "predicting on lightgbm\n",
      "predicting on lightgbm\n",
      "predicting on lightgbm\n",
      "predicting on lightgbm\n",
      "predicting on catboost\n",
      "predicting on catboost\n",
      "predicting on catboost\n",
      "predicting on catboost\n",
      "predicting on catboost\n",
      "predicting on lightgbm\n",
      "predicting on lightgbm\n",
      "predicting on lightgbm\n",
      "predicting on lightgbm\n",
      "predicting on lightgbm\n",
      "predicting on xgboost\n",
      "predicting on xgboost\n",
      "predicting on xgboost\n",
      "predicting on xgboost\n",
      "predicting on xgboost\n",
      "predicting on catboost\n",
      "predicting on catboost\n",
      "predicting on catboost\n",
      "predicting on catboost\n",
      "predicting on catboost\n",
      "predicting on lightgbm\n",
      "predicting on lightgbm\n",
      "predicting on lightgbm\n",
      "predicting on lightgbm\n",
      "predicting on lightgbm\n",
      "predicting on xgboost\n",
      "predicting on xgboost\n",
      "predicting on xgboost\n",
      "predicting on xgboost\n",
      "predicting on xgboost\n",
      "predicting on catboost\n",
      "predicting on catboost\n",
      "predicting on catboost\n",
      "predicting on catboost\n",
      "predicting on catboost\n",
      "predicting on xgboost\n",
      "predicting on xgboost\n",
      "predicting on xgboost\n",
      "predicting on xgboost\n",
      "predicting on xgboost\n",
      "predicting on catboost\n",
      "predicting on catboost\n",
      "predicting on catboost\n",
      "predicting on catboost\n",
      "predicting on catboost\n",
      "predicting on ensembler\n",
      "predicting on xgboostpredicting on lightgbm\n",
      "\n",
      "predicting on catboost\n",
      "predicting on catboost\n",
      "predicting on lightgbm\n",
      "predicting on catboost\n",
      "predicting on catboost\n",
      "predicting on lightgbmpredicting on catboost\n",
      "\n",
      "predicting on xgboost\n",
      "predicting on lightgbm\n",
      "predicting on lightgbm\n",
      "predicting on xgboost\n",
      "predicting on xgboost\n",
      "predicting on xgboost\n",
      "building prediction submission: \n",
      "Writing csv ...\n"
     ]
    }
   ],
   "source": [
    "from mlens.ensemble import SuperLearner\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import learning_curve, validation_curve\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor, BaggingRegressor\n",
    "from sklearn.linear_model import ElasticNet, Ridge, Lasso\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from scipy.stats import uniform, randint\n",
    "from mlens.preprocessing import EnsembleTransformer\n",
    "from mlens.metrics import make_scorer\n",
    "from mlens.model_selection import Evaluator\n",
    "from sklearn.base import BaseEstimator\n",
    "from model_super import *\n",
    "from models import *\n",
    "from ensembler import *\n",
    "from parameters import *\n",
    "from data_processing import *\n",
    "from gridSearch import *\n",
    "import os.path as path\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#results of iteration 1 (all models):  example.csv for params, 0.052445\n",
    "#results of iteration 2 (three models):  iteration2.csv for params, 0.052109\n",
    "#iteration 5 params gives 40th percentile on Kaggle -- best ensembling score yet\n",
    "\n",
    "# TODO:  Analyze different base models with gridsearch (RandomForest, Adaboost, Neural Networks, DecisionTree, Lasso)\n",
    "# TODO:  remember to scale neural network training data before feeding it\n",
    "\n",
    "############################################## Custom Catboost Class ###################################################\n",
    "\n",
    "class MultiCatBoost(BaseEstimator):\n",
    "\n",
    "    def __init__(self, parameters, cat_feature_inds):\n",
    "        self.cat_feature_inds = cat_feature_inds\n",
    "        self.models = []\n",
    "        self.parameters = parameters\n",
    "        for i in range(5):\n",
    "            self.models.append(CatBoostRegressor(**self.parameters, random_seed=i))\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "        for i in range(5):\n",
    "            self.models[i].fit(x_train, y_train, cat_features=self.cat_feature_inds)\n",
    "        return self\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        result = 0.0\n",
    "        for model in self.models:\n",
    "            print(\"predicting on catboost\")\n",
    "            result += model.predict(x_test, verbose=True)\n",
    "        result /= 5\n",
    "        return result\n",
    "\n",
    "\n",
    "class MultiXGBoost(BaseEstimator):\n",
    "\n",
    "    def __init__(self, parameters):\n",
    "        self.models = []\n",
    "        self.parameters = parameters\n",
    "        for i in range(5):\n",
    "            self.models.append(XGBRegressor(**self.parameters, random_state=i))\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "        for i in range(5):\n",
    "            self.models[i].fit(x_train, y_train)\n",
    "        return self\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        result = 0.0\n",
    "        for model in self.models:\n",
    "            print(\"predicting on xgboost\")\n",
    "            result += model.predict(x_test)\n",
    "        result /= 5\n",
    "        return result\n",
    "\n",
    "\n",
    "class MultiLightGBM(BaseEstimator):\n",
    "\n",
    "    def __init__(self, parameters):\n",
    "        self.models = []\n",
    "        self.parameters = parameters\n",
    "        for i in range(5):\n",
    "            self.models.append(LGBMRegressor(**self.parameters, random_state=i))\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "        for i in range(5):\n",
    "            self.models[i].fit(x_train, y_train)\n",
    "        return self\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        result = 0.0\n",
    "        for model in self.models:\n",
    "            print(\"predicting on lightgbm\")\n",
    "            result += model.predict(x_test)\n",
    "        result /= 5\n",
    "        return result\n",
    "\n",
    "############################################## Helper methods ##########################################################\n",
    "\n",
    "#Performs gridsearch on the \"meta-learners\" which predict on the first layer predictions\n",
    "def evaluateSecondLayer(base_learners, x_train, y_train, meta_learners, param_dicts):\n",
    "    in_layer = EnsembleTransformer()\n",
    "    print(\"adding base learners to transformer\")\n",
    "    in_layer.add('stack', base_learners)\n",
    "\n",
    "    preprocess = [in_layer]\n",
    "    print(\"creating scorer\")\n",
    "    scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "    evl = Evaluator(scorer, cv=4, verbose=1)\n",
    "    print(\"fitting evaluator\")\n",
    "    evl.fit(x_train.values,\n",
    "        y_train.values,\n",
    "        meta_learners,\n",
    "        param_dicts,\n",
    "        preprocessing={'meta': preprocess},\n",
    "        n_iter=30                            # bump this up to do a larger grid search\n",
    "       )\n",
    "\n",
    "    table = pd.DataFrame(evl.summary)\n",
    "    table.to_html('HyperCatboost.html')\n",
    "    table.to_csv('HypterCatboost.csv', index=False, header=False, sep='\\t')\n",
    "\n",
    "\n",
    "#Adds features to the dataset\n",
    "def add_date_features(df):\n",
    "    df[\"transaction_year\"] = df[\"transactiondate\"].dt.year\n",
    "    df[\"transaction_month\"] = df[\"transactiondate\"].dt.month\n",
    "    df[\"transaction_day\"] = df[\"transactiondate\"].dt.day\n",
    "    df[\"transaction_quarter\"] = df[\"transactiondate\"].dt.quarter\n",
    "    df.drop([\"transactiondate\"], inplace=True, axis=1)\n",
    "    return df\n",
    "\n",
    "def plot_learning_curve(estimator, x_train, y_train):\n",
    "    title = \"XGBoost as Second Layer Predictor\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    print(\"calculating learning curve values\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, x_train, y_train, n_jobs=-1, \n",
    "                scoring = 'neg_mean_absolute_error', cv=4)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "    \n",
    "    plt.legend(loc=\"best\")\n",
    "    print('showing plot')\n",
    "    plt.show()\n",
    "    print(test_scores_mean)\n",
    "\n",
    "def plot_validation_curve(estimator, x_train, y_train, param_name, param_range):\n",
    "    print('computing validation curve values')\n",
    "    train_scores, test_scores = validation_curve(estimator, x_train, y_train, param_name=param_name, \n",
    "                param_range=param_range, scoring='neg_mean_absolute_error', n_jobs=-1, cv=4)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.title(\"XGBoost colsample analysis\")\n",
    "    plt.xlabel(\"colsample\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    lw = 2\n",
    "    plt.plot(param_range, train_scores_mean, 'r+', label='Train')\n",
    "    plt.plot(param_range, test_scores_mean, 'g+', label='Test')\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(test_scores_mean)\n",
    "\n",
    "########################################### LOADING DATA ##############################################################\n",
    "\n",
    "dir_path = path.abspath(path.join('__file__',\"../../..\"))\n",
    "train_path = dir_path + '/train_2016_v2.csv'\n",
    "test_path = dir_path + '/submission.csv'\n",
    "properties_path = dir_path + '/properties_2016.csv'\n",
    "#first_layer_predictions_file = dir_path + '/predictions_first_layer.csv'\n",
    "\n",
    "print(train_path)\n",
    "\n",
    "train_df = pd.read_csv(train_path, parse_dates=['transactiondate'], low_memory=False)\n",
    "test_df = pd.read_csv(test_path, low_memory=False)\n",
    "properties = pd.read_csv(properties_path, low_memory=False)\n",
    "# field is named differently in submission\n",
    "test_df['parcelid'] = test_df['ParcelId']\n",
    "\n",
    "########################################## PROCESSING DATA ############################################################\n",
    "\n",
    "train_df = add_date_features(train_df)\n",
    "train_df = train_df.merge(properties, how='left', on='parcelid')\n",
    "test_df = test_df.merge(properties, how='left', on='parcelid')\n",
    "\n",
    "#Identify columns with many missing values and store them into a variable\n",
    "exclude_missing = missingValueColumns(train_df)\n",
    "\n",
    "# Identify columns with only one unique value and store them into a variable\n",
    "exclude_unique = nonUniqueColumns(train_df)\n",
    "\n",
    "#Identify columns that we will use for training and store them into a variable\n",
    "train_features = trainingColumns(train_df, exclude_missing, exclude_unique)\n",
    "\n",
    "#Identify categorical columns\n",
    "cat_feature_inds = categoricalColumns(train_df, train_features)\n",
    "\n",
    "# Handle NA values\n",
    "train_df.fillna(-1, inplace=True)\n",
    "test_df.fillna(-1, inplace=True)\n",
    "\n",
    "#Disregard outliers\n",
    "train_df = train_df[train_df.logerror > -0.4]\n",
    "train_df = train_df[train_df.logerror < 0.4]\n",
    "\n",
    "#Initialize training datasets\n",
    "x_train = train_df[train_features]\n",
    "y_train = train_df.logerror\n",
    "\n",
    "#Handle types so training does not throw errors\n",
    "for c in x_train.dtypes[x_train.dtypes == object].index.values:\n",
    "    x_train[c] = (x_train[c] == True)\n",
    "\n",
    "#Set up test dataset\n",
    "test_df['transactiondate'] = pd.Timestamp('2016-12-01')  # Dummy\n",
    "test_df = add_date_features(test_df)\n",
    "X_test = test_df[train_features]\n",
    "\n",
    "#Handle types so testing does not throw errors\n",
    "for c in X_test.dtypes[X_test.dtypes == object].index.values:\n",
    "    X_test[c] = (X_test[c] == True)\n",
    "\n",
    "first_layer_results = pd.read_csv('predictions_first_layer.csv')\n",
    "for c in first_layer_results.dtypes[first_layer_results.dtypes == object].index.values:\n",
    "    first_layer_results[c] = (first_layer_results[c] == True)    \n",
    "\n",
    "#x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=27000)\n",
    "\n",
    "#run grid search on base models to hypertune parameters\n",
    "#base_model_gridSearch(x_train, y_train)\n",
    "\n",
    "############################################# Graph Learning Curve ####################################################\n",
    "\n",
    "#estimator = Ridge(**ridge_params)\n",
    "#estimator = XGBRegressor(**xgb_params_2)\n",
    "#plot_learning_curve(estimator, first_layer_results, y_train)\n",
    "\n",
    "############################################ Graph Validation Curve ###################################################\n",
    "\n",
    "#value_range = [2, 3, 4, 5]\n",
    "#plot_validation_curve(estimator, first_layer_results, y_train,'max_depth', value_range)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "#GridSearch\n",
    "\n",
    "\n",
    "#xgb_mod = MultiXGBoost(getXGBParams(y_train))\n",
    "#lgbm_mod = MultiLightGBM(lightGBM_params)\n",
    "#cat_mod = MultiCatBoost(catboost_params, cat_feature_inds)\n",
    "#models = [xgb_mod, lgbm_mod, cat_mod]\n",
    "\n",
    "\n",
    "def generate_first_layer_predictions():\n",
    "    x_train = np.array(x_train)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    kf = KFold(n_splits=4)\n",
    "    folds = list(kf.split(x_train, y_train))\n",
    "\n",
    "    first_layer_train_predictions = np.zeros((x_train.shape[0], len(models)))\n",
    "\n",
    "    #train first layer\n",
    "    for i in range(len(models)):\n",
    "        print(\"training baseline model\")\n",
    "        for j, (train_idx, test_idx) in enumerate(folds):\n",
    "            x_train_fold = x_train[train_idx]\n",
    "            y_train_fold = y_train[train_idx]\n",
    "            x_holdout_fold = x_train[test_idx]\n",
    "            y_holdout_fold = y_train[test_idx]\n",
    "            models[i].fit(x_train_fold, y_train_fold)\n",
    "            first_layer_train_predictions[test_idx, i] = models[i].predict(x_holdout_fold)\n",
    "\n",
    "    print(\"first layer train predictions: \")\n",
    "    print(first_layer_train_predictions)\n",
    "    print(\"shape: \")\n",
    "    print(first_layer_train_predictions.shape)\n",
    "    print(\"building csv\")\n",
    "    np.savetxt(\"predictions_first_layer.csv\", first_layer_train_predictions, delimiter=\",\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "xgb_mod = XGBRegressor(**xgb_params_2)\n",
    "ridge = Ridge()\n",
    "lasso = Lasso()\n",
    "\n",
    "base_learners = [('xgb_2', xgb_mod), ('Ridge', ridge), ('Lasso', lasso)]\n",
    "\n",
    "param_dicts = {'xgb_2': \n",
    "               {'learning_rate': uniform(0.01, 0.12),\n",
    "                    'subsample': uniform(0.8, 0.2),\n",
    "                    'reg_lambda': uniform(0.4, 5),\n",
    "                    'max_depth': randint(2, 5),\n",
    "                    'reg_alpha': uniform(0.1, 4),\n",
    "                    'n_estimators': randint(50, 400),\n",
    "                    'colsample_bytree': uniform(0.8, 0.2)},\n",
    "               'Ridge':\n",
    "                  {'alpha': uniform(0.5, 15),\n",
    "                   'max_iter': randint(1000, 12000),\n",
    "                   'tol': uniform(0.00003, 0.0003),\n",
    "                   },\n",
    "               'Lasso':\n",
    "                   {'alpha': uniform(0.5, 15),\n",
    "                    'max_iter': randint(1000, 12000),\n",
    "                    'tol': uniform(0.0003, 0.002),\n",
    "                  }\n",
    "              }\n",
    "\n",
    "scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "evl = Evaluator(scorer,cv=4,verbose=True)\n",
    "\n",
    "first_layer_results = pd.read_csv('predictions_first_layer.csv')\n",
    "for c in first_layer_results.dtypes[first_layer_results.dtypes == object].index.values:\n",
    "    first_layer_results[c] = (first_layer_results[c] == True)\n",
    "\n",
    "print(\"shape: \")\n",
    "print(first_layer_results.shape)\n",
    "\n",
    "\n",
    "print('fitting')\n",
    "evl.fit(first_layer_results.values,  \n",
    "            y_train.values,\n",
    "            estimators=base_learners,\n",
    "            param_dicts=param_dicts,\n",
    "            n_iter=100)  # bump this up to do a larger grid search\n",
    "\n",
    "table = pd.DataFrame(evl.summary)\n",
    "table.to_html('XGBRegressor_2.html')\n",
    "table.to_csv('XGBRegressor_2.csv', index=False, header=False, sep='\\t') \"\"\"\n",
    "                \n",
    "\"\"\"\n",
    "param_dicts = {'Ridge':\n",
    "                  {'alpha': uniform(0.5, 15),\n",
    "                   'max_iter': randint(1000, 12000),\n",
    "                   'tol': uniform(0.00003, 0.0003),\n",
    "                   },\n",
    "               'Lasso':\n",
    "                   {'alpha': uniform(0.5, 15),\n",
    "                    'max_iter': randint(1000, 12000),\n",
    "                    'tol': uniform(0.0003, 0.002),\n",
    "                  }\n",
    "              } \"\"\"\n",
    "\n",
    "#evaluateSecondLayer(base_learners, x_train, y_train, meta_learners, param_dicts)\n",
    "\n",
    "\n",
    "########################################## Create and Train Ensembler ##################################################\n",
    "\n",
    "\n",
    "\n",
    "ensemble = SuperLearner(folds=4)\n",
    "\n",
    "print(\"adding baseline models to ensembler\")\n",
    "\n",
    "ensemble.add([MultiXGBoost(getXGBParams(y_train)), MultiLightGBM(lightGBM_params),\n",
    "              MultiCatBoost(catboost_params, cat_feature_inds)])\n",
    "\n",
    "#ensemble.add_meta(XGBRegressor(**xgb_params_2))\n",
    "ensemble.add_meta(Ridge(**ridge_params))\n",
    "\n",
    "print(\"training ensembler\")\n",
    "ensemble.fit(x_train, y_train)\n",
    "\n",
    "######################################### PREDICTING ON ENSEMBLE #######################################################\n",
    "\n",
    "print(\"predicting on ensembler\")\n",
    "preds = ensemble.predict(X_test)\n",
    "\n",
    "\n",
    "\"\"\"\"#Validation prediction:\n",
    "\n",
    "preds = ensemble.predict(x_val)\n",
    "accuracy = mean_absolute_error(y_val, preds)\n",
    "print('validation accuracy: ')\n",
    "print(accuracy) \"\"\"\n",
    "\n",
    "######################################### BUILDING KAGGLE SUBMISSION ###################################################\n",
    "\n",
    "\n",
    "print(\"building prediction submission: \")\n",
    "sub = pd.read_csv(test_path)\n",
    "for c in sub.columns[sub.columns != 'ParcelId']:\n",
    "    sub[c] = preds\n",
    "\n",
    "print('Writing csv ...')\n",
    "sub.to_csv('kaggle_submission.csv', index=False, float_format='%.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
