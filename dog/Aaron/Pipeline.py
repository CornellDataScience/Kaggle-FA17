import numpy as np #linalg
import pandas as pd #IO

import os
from tqdm import tqdm
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
import cv2

from os import listdir, makedirs, getcwd, remove
from os.path import isfile, join, abspath, exists, isdir, expanduser
from shutil import copy2

import datetime
import re

#Using TensorFlow 
import keras

# VGG19 is slower but more accurate than VGG16
from keras.applications.vgg19 import VGG19 
from keras.models import Model
from keras.layers import Dense, Dropout, Flatten

#Read in Keras Data Set
#Read data from https://www.kaggle.com/gaborfodor/keras-pretrained-models
#(need to use data set in the kernel)
#print("\nReading in Keras Training Data...")
#print(listdir("../input/keras-pretrained-models/"))
#cache_dir = expanduser(join('~', '.keras'))
#if not exists(cache_dir):
#    makedirs(cache_dir)
#models_dir = join(cache_dir, 'models')
#if not exists(models_dir):
#    makedirs(models_dir)
#copy2("../input/keras-pretrained-models/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5", models_dir)
#print(listdir(models_dir))

#Reading
print("\nReading Files")
df_train = pd.read_csv('../labels.csv')
df_test = pd.read_csv('../sample_submission.csv')

#Format data into sample submission format
print("\nFormatting Data and Submission Type")
targets_series = pd.Series(df_train['breed'])
one_hot = pd.get_dummies(targets_series, sparse = True)
one_hot_labels = np.asarray(one_hot)

##### Training

#Set the Image Rescale size
print("\nBuilding Training Test...")
im_size = 90

#Build the training arrays
x_train = []
y_train = []
x_test = []

i = 0 
for f, breed in tqdm(df_train.values):
    img = cv2.imread('../train/{}.jpg'.format(f))
    label = one_hot_labels[i]
    x_train.append(cv2.resize(img, (im_size, im_size)))
    y_train.append(label)
    i += 1
    
for f in tqdm(df_test['id'].values):
    img = cv2.imread('../test/{}.jpg'.format(f))
    x_test.append(cv2.resize(img, (im_size, im_size)))
    
y_train_raw = np.array(y_train, np.uint8)
x_train_raw = np.array(x_train, np.float32) / 255.
x_test  = np.array(x_test, np.float32) / 255.

num_class = y_train_raw.shape[1]

X_train, X_valid, Y_train, Y_valid = train_test_split(x_train_raw, y_train_raw, test_size=0.3, random_state=1)

print("\n Done")
print("\n\nBuilding Model...")

base_model = VGG19(weights='imagenet',include_top=False, input_shape=(im_size, im_size, 3))
x = base_model.output
x = Flatten()(x)
predictions = Dense(num_class, activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=predictions)
for layer in base_model.layers:
    layer.trainable = False

print("\nTraining Model...")

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
#callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, verbose=1)]
model.summary()

model.fit(X_train, Y_train, epochs=20, validation_data=(X_valid, Y_valid), verbose=1)

print("\nPredicting")
preds = model.predict(x_test, verbose=1)
sub = pd.DataFrame(preds)

# Set column names to those generated by the one-hot encoding earlier
col_names = one_hot.columns.values
sub.columns = col_names
# Insert the column id from the sample_submission at the start of the data frame
sub.insert(0, 'id', df_test['id'])
sub.head(5)

# Write to file
print("\n Printing to Disk")

filename = "Prediction_Dog_Classif_" + re.sub("[^0-9]", "",str(datetime.datetime.now())) + '.csv'
print(filename)
sub.to_csv(filename,index=False)