{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "# 12:07\n",
    "\n",
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Training packages\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Ensemble\n",
    "# Is it true that you can just put sklearn compatible packages into this?\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
    "\n",
    "# Cross Validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions that might be useful\n",
    "\n",
    "# 01 Gini coefficient calculations\n",
    "# https://www.kaggle.com/c/ClaimPredictionChallenge/discussion/703\n",
    "def gini(actual, pred, cmpcol = 0, sortcol = 1):\n",
    "    assert( len(actual) == len(pred) )\n",
    "    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n",
    "    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n",
    "    totalLosses = all[:,0].sum()\n",
    "    giniSum = all[:,0].cumsum().sum() / totalLosses\n",
    " \n",
    "    giniSum -= (len(actual) + 1) / 2.\n",
    "    return giniSum / len(actual)\n",
    " \n",
    "def gini_normalized(a, p):\n",
    "    return gini(a, p) / gini(a, a)\n",
    "\n",
    "# 02 Gini coefficient for xgb and lgb\n",
    "# https://www.kaggle.com/rshally/porto-xgb-lgb-kfold-lb-0-282  (Version 1)\n",
    "def gini_xgb(pred, y):\n",
    "    y = y.get_label()\n",
    "    return 'gini', gini(y, pred) / gini(y, y)\n",
    "\n",
    "def gini_lgb(preds, dtrain):\n",
    "    y = list(dtrain.get_label())\n",
    "    score = gini(y, preds) / gini(y, y)\n",
    "    return 'gini', score, True\n",
    "\n",
    "# 02 The Ensemble function\n",
    "# https://www.kaggle.com/yekenot/simple-stacker-lb-0-284/code (Version 8)\n",
    "class Ensemble(object):\n",
    "    def __init__(self, n_splits, stacker, base_models):\n",
    "        self.n_splits = n_splits\n",
    "        self.stacker = stacker\n",
    "        self.base_models = base_models\n",
    "\n",
    "    def fit_predict(self, X, y, T):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        T = np.array(T)\n",
    "\n",
    "        folds = list(StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=2016).split(X, y))\n",
    "\n",
    "        S_train = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        S_test = np.zeros((T.shape[0], len(self.base_models)))\n",
    "        for i, clf in enumerate(self.base_models):\n",
    "\n",
    "            S_test_i = np.zeros((T.shape[0], self.n_splits))\n",
    "\n",
    "            for j, (train_idx, test_idx) in enumerate(folds):\n",
    "                X_train = X[train_idx]\n",
    "                y_train = y[train_idx]\n",
    "                X_holdout = X[test_idx]\n",
    "#                y_holdout = y[test_idx]\n",
    "\n",
    "                print (\"Fit %s fold %d\" % (str(clf).split('(')[0], j+1))\n",
    "                clf.fit(X_train, y_train)\n",
    "#                cross_score = cross_val_score(clf, X_train, y_train, cv=3, scoring='roc_auc')\n",
    "#                print(\"    cross_score: %.5f\" % (cross_score.mean()))\n",
    "                y_pred = clf.predict_proba(X_holdout)[:,1]                \n",
    "\n",
    "                S_train[test_idx, i] = y_pred\n",
    "                S_test_i[:, j] = clf.predict_proba(T)[:,1]\n",
    "            S_test[:, i] = S_test_i.mean(axis=1)\n",
    "\n",
    "        results = cross_val_score(self.stacker, S_train, y, cv=3, scoring='roc_auc')\n",
    "        print(\"Stacker score: %.5f\" % (results.mean()))\n",
    "\n",
    "        self.stacker.fit(S_train, y)\n",
    "        res = self.stacker.predict_proba(S_test)[:,1]\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading files...\n",
      "(595212, 59) (892816, 58)\n"
     ]
    }
   ],
   "source": [
    "# Loading Files and Picking out NA values\n",
    "# It seems that if we don't pick out NA values, there will be one missing value in the id column\n",
    "print('loading files...')\n",
    "train = pd.read_csv('/Users/maxji/Desktop/Kaggle/0SafeDriver/data/train.csv', na_values=-1)\n",
    "test = pd.read_csv('/Users/maxji/Desktop/Kaggle/0SafeDriver/data/test.csv', na_values=-1)\n",
    "\n",
    "# Change format to reduce memory usage\n",
    "for c in train.select_dtypes(include=['float64']).columns:\n",
    "    train[c]=train[c].astype(np.float32)\n",
    "    test[c]=test[c].astype(np.float32)\n",
    "for c in train.select_dtypes(include=['int64']).columns[2:]:\n",
    "    train[c]=train[c].astype(np.int8)\n",
    "    test[c]=test[c].astype(np.int8)  \n",
    "\n",
    "# Print out the shape of train and test\n",
    "print(train.shape,test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ps_calc_01', 'ps_calc_02', 'ps_calc_03', 'ps_calc_04', 'ps_calc_05',\n",
      "       'ps_calc_06', 'ps_calc_07', 'ps_calc_08', 'ps_calc_09', 'ps_calc_10',\n",
      "       'ps_calc_11', 'ps_calc_12', 'ps_calc_13', 'ps_calc_14',\n",
      "       'ps_calc_15_bin', 'ps_calc_16_bin', 'ps_calc_17_bin', 'ps_calc_18_bin',\n",
      "       'ps_calc_19_bin', 'ps_calc_20_bin'],\n",
      "      dtype='object')\n",
      "(595212, 200) (892816, 199)\n"
     ]
    }
   ],
   "source": [
    "# Data Manipulation\n",
    "\n",
    "# 01 Dropping Columns starting with Calc\n",
    "# Note: This is used by almost all kernels online. \n",
    "# Justification: https://www.kaggle.com/arthurtok/interactive-porto-insights-a-plot-ly-tutorial\n",
    "# At least in Gradient Boosting, the calc variables all show really low correlation with target.\n",
    "\n",
    "col_to_drop = train.columns[train.columns.str.startswith('ps_calc_')]\n",
    "print(col_to_drop)\n",
    "train = train.drop(col_to_drop, axis=1)  \n",
    "test = test.drop(col_to_drop, axis=1) \n",
    "\n",
    "# 02 Dropping more Columns:\n",
    "# Justification: https://www.kaggle.com/arthurtok/interactive-porto-insights-a-plot-ly-tutorial\n",
    "# Trying to drop more columns with less weight in Gradient Boosting test.\n",
    "\n",
    "col_to_drop_2 = [a for a in train.columns if a.endswith('ps_ind_1')]\n",
    "train = train.drop(col_to_drop_2, axis=1)  \n",
    "test = test.drop(col_to_drop_2, axis=1) \n",
    "\n",
    "# 03 Treating missing values:\n",
    "# Again, different ways are employed by different Kernels.\n",
    "# a. Kernels that doesn't treat values (Keep NA in the data)\n",
    "# https://www.kaggle.com/rshally/porto-xgb-lgb-kfold-lb-0-282  (Version 1)\n",
    "# b. Kernels that keep NA values as -1\n",
    "# c. Kernels that change NA values to 999/-999\n",
    "\n",
    "# 04 Dealing with categorical variables\n",
    "# a. Make up dummy variables for each of them\n",
    "cat_features = [a for a in train.columns if a.endswith('cat')]\n",
    "for column in cat_features:\n",
    "    temp = pd.get_dummies(pd.Series(train[column]))\n",
    "    train = pd.concat([train,temp],axis=1)\n",
    "    train = train.drop([column],axis=1)\n",
    "    \n",
    "for column in cat_features:\n",
    "    temp = pd.get_dummies(pd.Series(test[column]))\n",
    "    test = pd.concat([test,temp],axis=1)\n",
    "    test = test.drop([column],axis=1)\n",
    "\n",
    "print(train.shape,test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>ps_ind_01</th>\n",
       "      <th>ps_ind_03</th>\n",
       "      <th>ps_ind_06_bin</th>\n",
       "      <th>ps_ind_07_bin</th>\n",
       "      <th>ps_ind_08_bin</th>\n",
       "      <th>ps_ind_09_bin</th>\n",
       "      <th>ps_ind_10_bin</th>\n",
       "      <th>ps_ind_11_bin</th>\n",
       "      <th>...</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.952120e+05</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.438036e+05</td>\n",
       "      <td>0.036448</td>\n",
       "      <td>1.900378</td>\n",
       "      <td>4.423318</td>\n",
       "      <td>0.393742</td>\n",
       "      <td>0.257033</td>\n",
       "      <td>0.163921</td>\n",
       "      <td>0.185304</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>0.001692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005978</td>\n",
       "      <td>0.003483</td>\n",
       "      <td>0.002493</td>\n",
       "      <td>0.004788</td>\n",
       "      <td>0.020231</td>\n",
       "      <td>0.007468</td>\n",
       "      <td>0.012330</td>\n",
       "      <td>0.003533</td>\n",
       "      <td>0.040762</td>\n",
       "      <td>0.142946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.293678e+05</td>\n",
       "      <td>0.187401</td>\n",
       "      <td>1.983789</td>\n",
       "      <td>2.699902</td>\n",
       "      <td>0.488579</td>\n",
       "      <td>0.436998</td>\n",
       "      <td>0.370205</td>\n",
       "      <td>0.388544</td>\n",
       "      <td>0.019309</td>\n",
       "      <td>0.041097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077084</td>\n",
       "      <td>0.058912</td>\n",
       "      <td>0.049870</td>\n",
       "      <td>0.069031</td>\n",
       "      <td>0.140791</td>\n",
       "      <td>0.086094</td>\n",
       "      <td>0.110354</td>\n",
       "      <td>0.059336</td>\n",
       "      <td>0.197738</td>\n",
       "      <td>0.350018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.719915e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.435475e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.115549e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.488027e+06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id         target      ps_ind_01      ps_ind_03  \\\n",
       "count  5.952120e+05  595212.000000  595212.000000  595212.000000   \n",
       "mean   7.438036e+05       0.036448       1.900378       4.423318   \n",
       "std    4.293678e+05       0.187401       1.983789       2.699902   \n",
       "min    7.000000e+00       0.000000       0.000000       0.000000   \n",
       "25%    3.719915e+05       0.000000       0.000000       2.000000   \n",
       "50%    7.435475e+05       0.000000       1.000000       4.000000   \n",
       "75%    1.115549e+06       0.000000       3.000000       6.000000   \n",
       "max    1.488027e+06       1.000000       7.000000      11.000000   \n",
       "\n",
       "       ps_ind_06_bin  ps_ind_07_bin  ps_ind_08_bin  ps_ind_09_bin  \\\n",
       "count  595212.000000  595212.000000  595212.000000  595212.000000   \n",
       "mean        0.393742       0.257033       0.163921       0.185304   \n",
       "std         0.488579       0.436998       0.370205       0.388544   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         1.000000       1.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "       ps_ind_10_bin  ps_ind_11_bin      ...                   95  \\\n",
       "count  595212.000000  595212.000000      ...        595212.000000   \n",
       "mean        0.000373       0.001692      ...             0.005978   \n",
       "std         0.019309       0.041097      ...             0.077084   \n",
       "min         0.000000       0.000000      ...             0.000000   \n",
       "25%         0.000000       0.000000      ...             0.000000   \n",
       "50%         0.000000       0.000000      ...             0.000000   \n",
       "75%         0.000000       0.000000      ...             0.000000   \n",
       "max         1.000000       1.000000      ...             1.000000   \n",
       "\n",
       "                  96             97             98             99  \\\n",
       "count  595212.000000  595212.000000  595212.000000  595212.000000   \n",
       "mean        0.003483       0.002493       0.004788       0.020231   \n",
       "std         0.058912       0.049870       0.069031       0.140791   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                 100            101            102            103  \\\n",
       "count  595212.000000  595212.000000  595212.000000  595212.000000   \n",
       "mean        0.007468       0.012330       0.003533       0.040762   \n",
       "std         0.086094       0.110354       0.059336       0.197738   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                 104  \n",
       "count  595212.000000  \n",
       "mean        0.142946  \n",
       "std         0.350018  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Preparation for Training\n",
    "# 01 Dropping columns, separating target function (y) and features (X)\n",
    "#  X: The values of feature dataframe\n",
    "#  y: target dataframe\n",
    "#  features: The columns of feature dataframe\n",
    "X = train.drop(['id', 'target'], axis=1)\n",
    "features = X.columns\n",
    "X = X.values\n",
    "y = train['target'].values\n",
    "\n",
    "# 02 Create and prepare the submission dataset\n",
    "#  sub: The submission dataframe \n",
    "sub=test['id'].to_frame()\n",
    "sub['target']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training parameters \n",
    "# 01 xgboost\n",
    "# a. Baseline without feature engineering\n",
    "# https://www.kaggle.com/rshally/porto-xgb-lgb-kfold-lb-0-282   (Version 1)\n",
    "\"\"\"params = {'eta': 0.02, 'max_depth': 4, 'subsample': 0.9, 'colsample_bytree': 0.9, \n",
    "          'objective': 'binary:logistic', 'eval_metric': 'auc', 'silent': True}\"\"\"\n",
    "# local cv: 0.2848016, lb: 0.281\n",
    "\n",
    "# b. XGBoost params with one-hot encoding\n",
    "# https://www.kaggle.com/yekenot/simple-stacker-lb-0-284/code (Version 8)\n",
    "xgb_params = {}\n",
    "xgb_params['objective'] = 'binary:logistic'\n",
    "xgb_params['learning_rate'] = 0.04\n",
    "xgb_params['n_estimators'] = 490\n",
    "xgb_params['max_depth'] = 4\n",
    "xgb_params['subsample'] = 0.9\n",
    "xgb_params['colsample_bytree'] = 0.9  \n",
    "xgb_params['min_child_weight'] = 10\n",
    "\n",
    "\n",
    "# 02 lightgbm\n",
    "# a. Baseline without feature engineering\n",
    "# https://www.kaggle.com/rshally/porto-xgb-lgb-kfold-lb-0-282   (Version 1)\n",
    "\n",
    "# @鲲(China) lgbm is very sensitive with hyper parameters, my lgbm give me 0.281. Here's my suggestion, \n",
    "# use a small max_depth and a num_of_leaves smaller than 2**max_depth, also try bagging with a small bagging frequency\n",
    "\"\"\"params = {'metric': 'auc', 'learning_rate' : 0.01, 'max_depth':10, 'max_bin':10,  'objective': 'binary', \n",
    "          'feature_fraction': 0.8,'bagging_fraction':0.9,'bagging_freq':10,  'min_data': 500}\"\"\"\n",
    "# local cv: 0.284789, lb: 0.281\n",
    "# with dummy variables: local cv: 0.2852834 lb: 0.206 This is inconsistent\n",
    "\n",
    "# b. With one-hot encoding\n",
    "# https://www.kaggle.com/yekenot/simple-stacker-lb-0-284/code (Version 8)\n",
    "# LightGBM params ver1\n",
    "lgb_params = {}\n",
    "lgb_params['learning_rate'] = 0.02\n",
    "lgb_params['n_estimators'] = 650\n",
    "lgb_params['max_bin'] = 10\n",
    "lgb_params['subsample'] = 0.8\n",
    "lgb_params['subsample_freq'] = 10\n",
    "lgb_params['colsample_bytree'] = 0.8   \n",
    "lgb_params['min_child_samples'] = 500\n",
    "lgb_params['seed'] = 99\n",
    "\n",
    "# https://www.kaggle.com/yekenot/simple-stacker-lb-0-284/code (Version 8)\n",
    "# LightGBM params ver2\n",
    "lgb_params2 = {}\n",
    "lgb_params2['n_estimators'] = 1090\n",
    "lgb_params2['learning_rate'] = 0.02\n",
    "lgb_params2['colsample_bytree'] = 0.3   \n",
    "lgb_params2['subsample'] = 0.7\n",
    "lgb_params2['subsample_freq'] = 2\n",
    "lgb_params2['num_leaves'] = 16\n",
    "lgb_params2['seed'] = 99\n",
    "\n",
    "# https://www.kaggle.com/yekenot/simple-stacker-lb-0-284/code (Version 8)\n",
    "# LightGBM params ver3\n",
    "lgb_params3 = {}\n",
    "lgb_params3['n_estimators'] = 1100\n",
    "lgb_params3['max_depth'] = 4\n",
    "lgb_params3['learning_rate'] = 0.02\n",
    "lgb_params3['seed'] = 99\n",
    "\n",
    "\n",
    "# 04 RandomForest params with one-hot encoding\n",
    "#rf_params = {}\n",
    "#rf_params['n_estimators'] = 200\n",
    "#rf_params['max_depth'] = 6\n",
    "#rf_params['min_samples_split'] = 70\n",
    "#rf_params['min_samples_leaf'] = 30\n",
    "\n",
    "\n",
    "# 05 ExtraTrees params with one-hot encoding\n",
    "#et_params = {}\n",
    "#et_params['n_estimators'] = 155\n",
    "#et_params['max_features'] = 0.3\n",
    "#et_params['max_depth'] = 6\n",
    "#et_params['min_samples_split'] = 40\n",
    "#et_params['min_samples_leaf'] = 18\n",
    "\n",
    "# 06 CatBoost params with one-hot encoding\n",
    "cat_params = {}\n",
    "cat_params['iterations'] = 900\n",
    "cat_params['depth'] = 8\n",
    "cat_params['rsm'] = 0.95\n",
    "cat_params['learning_rate'] = 0.03\n",
    "cat_params['l2_leaf_reg'] = 3.5  \n",
    "cat_params['border_count'] = 8\n",
    "cat_params['gradient_iterations'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# There has been some arguments on both sides for KFold and Stratified KFold. Need to investigate more.\n",
    "# Discussion:\n",
    "# @KALE I am now using Stratified 5Fold for my cv. But lgb cv score doesn't seem consistent with lb score. \n",
    "# lgb 0.282 - lb 0.279; lgb 0.281 - lb 0.280\n",
    "# @鲲(China) 3 fold without Stratified is very consistent with lb in my side\n",
    "\n",
    "# Personally, I feel that Stratified Kfold should be better, because the dataset has a imbalanced target, \n",
    "# and Stratified Kfold will try to balance out the different classes (0,1) in the target.\n",
    "\n",
    "# https://www.kaggle.com/rshally/porto-xgb-lgb-kfold-lb-0-282   (Version 1)\n",
    "nrounds=2000 \n",
    "kfold = 2\n",
    "skf = StratifiedKFold(n_splits=kfold, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for i, (train_index, test_index) in enumerate(skf.split(X, y)):\\n    print(' xgb kfold: {}  of  {} : '.format(i+1, kfold))\\n    X_train, X_valid = X[train_index], X[test_index]\\n    y_train, y_valid = y[train_index], y[test_index]\\n    \\n    d_train = xgb.DMatrix(X_train, y_train) \\n    d_valid = xgb.DMatrix(X_valid, y_valid) \\n    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\\n    xgb_model = xgb.train(params, d_train, nrounds, watchlist, early_stopping_rounds=100, \\n                          feval=gini_xgb, maximize=True, verbose_eval=100)\\n    prediction = xgb_model.predict(xgb.DMatrix(test[features].values), \\n                        ntree_limit=xgb_model.best_ntree_limit+50) / (kfold)\\n    sub['target'] += prediction\\n    \\ngc.collect()\\nsub.head(2)\""
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Actual Training\n",
    "# 01 xgboost\n",
    "# https://www.kaggle.com/rshally/porto-xgb-lgb-kfold-lb-0-282   (Version 1)\n",
    "# Running time: ~45min\n",
    "\n",
    "# About xgb_model.best_ntree_limit+50: \n",
    "# @Rudolph The credit for that goes to The1owl - it is not imperative but it seems to improve things a bit.\n",
    "# @David Yang xgb_model.best_ntree_limit+50 seems to be unnecessary but it may get a good result.I think this way is useful to lgb, too. \n",
    "# You can change the +n/-n just like a parameter if it would imporove your result.\n",
    "\"\"\"for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    print(' xgb kfold: {}  of  {} : '.format(i+1, kfold))\n",
    "    X_train, X_valid = X[train_index], X[test_index]\n",
    "    y_train, y_valid = y[train_index], y[test_index]\n",
    "    \n",
    "    d_train = xgb.DMatrix(X_train, y_train) \n",
    "    d_valid = xgb.DMatrix(X_valid, y_valid) \n",
    "    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "    xgb_model = xgb.train(params, d_train, nrounds, watchlist, early_stopping_rounds=100, \n",
    "                          feval=gini_xgb, maximize=True, verbose_eval=100)\n",
    "    prediction = xgb_model.predict(xgb.DMatrix(test[features].values), \n",
    "                        ntree_limit=xgb_model.best_ntree_limit+50) / (kfold)\n",
    "    sub['target'] += prediction\n",
    "    \n",
    "gc.collect()\n",
    "sub.head(2)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"skf = StratifiedKFold(n_splits=kfold, random_state=1)\\nfor i, (train_index, test_index) in enumerate(skf.split(X, y)):\\n    print(' lgb kfold: {}  of  {} : '.format(i+1, kfold))\\n    X_train, X_eval = X[train_index], X[test_index]\\n    y_train, y_eval = y[train_index], y[test_index]\\n    lgb_model = lgb.train(lgb_params, lgb.Dataset(X_train, label=y_train), nrounds, \\n                  lgb.Dataset(X_eval, label=y_eval), verbose_eval=10, \\n                  feval=gini_lgb, early_stopping_rounds=100)\\n    sub['target'] += lgb_model.predict(test[features].values, \\n                        num_iteration=lgb_model.best_iteration) / (kfold)\\n\""
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 02 LightGBM\n",
    "# https://www.kaggle.com/rshally/porto-xgb-lgb-kfold-lb-0-282   (Version 1)\n",
    "# Running Time:\n",
    "\"\"\"skf = StratifiedKFold(n_splits=kfold, random_state=1)\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    print(' lgb kfold: {}  of  {} : '.format(i+1, kfold))\n",
    "    X_train, X_eval = X[train_index], X[test_index]\n",
    "    y_train, y_eval = y[train_index], y[test_index]\n",
    "    lgb_model = lgb.train(lgb_params, lgb.Dataset(X_train, label=y_train), nrounds, \n",
    "                  lgb.Dataset(X_eval, label=y_eval), verbose_eval=10, \n",
    "                  feval=gini_lgb, early_stopping_rounds=100)\n",
    "    sub['target'] += lgb_model.predict(test[features].values, \n",
    "                        num_iteration=lgb_model.best_iteration) / (kfold)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 03 Trainging various other models\n",
    "# https://www.kaggle.com/yekenot/simple-stacker-lb-0-284/code (Version 8)\n",
    "lgb_model = LGBMClassifier(**lgb_params)\n",
    "lgb_model2 = LGBMClassifier(**lgb_params2)\n",
    "lgb_model3 = LGBMClassifier(**lgb_params3)\n",
    "#rf_model = RandomForestClassifier(**rf_params)\n",
    "#et_model = ExtraTreesClassifier(**et_params)\n",
    "xgb_model = XGBClassifier(**xgb_params)\n",
    "#cat_model = CatBoostClassifier(**cat_params)\n",
    "#gb_model = GradientBoostingClassifier(max_depth=5)\n",
    "#ada_model = AdaBoostClassifier()\n",
    "log_model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:282: LGBMDeprecationWarning: The `seed` parameter is deprecated and will be removed in next version. Please use `random_state` instead.\n",
      "  'Please use `random_state` instead.', LGBMDeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:282: LGBMDeprecationWarning: The `seed` parameter is deprecated and will be removed in next version. Please use `random_state` instead.\n",
      "  'Please use `random_state` instead.', LGBMDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit LGBMClassifier fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:282: LGBMDeprecationWarning: The `seed` parameter is deprecated and will be removed in next version. Please use `random_state` instead.\n",
      "  'Please use `random_state` instead.', LGBMDeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:282: LGBMDeprecationWarning: The `seed` parameter is deprecated and will be removed in next version. Please use `random_state` instead.\n",
      "  'Please use `random_state` instead.', LGBMDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit LGBMClassifier fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:282: LGBMDeprecationWarning: The `seed` parameter is deprecated and will be removed in next version. Please use `random_state` instead.\n",
      "  'Please use `random_state` instead.', LGBMDeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:282: LGBMDeprecationWarning: The `seed` parameter is deprecated and will be removed in next version. Please use `random_state` instead.\n",
      "  'Please use `random_state` instead.', LGBMDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit LGBMClassifier fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:282: LGBMDeprecationWarning: The `seed` parameter is deprecated and will be removed in next version. Please use `random_state` instead.\n",
      "  'Please use `random_state` instead.', LGBMDeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:282: LGBMDeprecationWarning: The `seed` parameter is deprecated and will be removed in next version. Please use `random_state` instead.\n",
      "  'Please use `random_state` instead.', LGBMDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit LGBMClassifier fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:282: LGBMDeprecationWarning: The `seed` parameter is deprecated and will be removed in next version. Please use `random_state` instead.\n",
      "  'Please use `random_state` instead.', LGBMDeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:282: LGBMDeprecationWarning: The `seed` parameter is deprecated and will be removed in next version. Please use `random_state` instead.\n",
      "  'Please use `random_state` instead.', LGBMDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit LGBMClassifier fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:282: LGBMDeprecationWarning: The `seed` parameter is deprecated and will be removed in next version. Please use `random_state` instead.\n",
      "  'Please use `random_state` instead.', LGBMDeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:282: LGBMDeprecationWarning: The `seed` parameter is deprecated and will be removed in next version. Please use `random_state` instead.\n",
      "  'Please use `random_state` instead.', LGBMDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit LGBMClassifier fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:282: LGBMDeprecationWarning: The `seed` parameter is deprecated and will be removed in next version. Please use `random_state` instead.\n",
      "  'Please use `random_state` instead.', LGBMDeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:282: LGBMDeprecationWarning: The `seed` parameter is deprecated and will be removed in next version. Please use `random_state` instead.\n",
      "  'Please use `random_state` instead.', LGBMDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit LGBMClassifier fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:282: LGBMDeprecationWarning: The `seed` parameter is deprecated and will be removed in next version. Please use `random_state` instead.\n",
      "  'Please use `random_state` instead.', LGBMDeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:282: LGBMDeprecationWarning: The `seed` parameter is deprecated and will be removed in next version. Please use `random_state` instead.\n",
      "  'Please use `random_state` instead.', LGBMDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit LGBMClassifier fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:282: LGBMDeprecationWarning: The `seed` parameter is deprecated and will be removed in next version. Please use `random_state` instead.\n",
      "  'Please use `random_state` instead.', LGBMDeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:282: LGBMDeprecationWarning: The `seed` parameter is deprecated and will be removed in next version. Please use `random_state` instead.\n",
      "  'Please use `random_state` instead.', LGBMDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit LGBMClassifier fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:282: LGBMDeprecationWarning: The `seed` parameter is deprecated and will be removed in next version. Please use `random_state` instead.\n",
      "  'Please use `random_state` instead.', LGBMDeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:282: LGBMDeprecationWarning: The `seed` parameter is deprecated and will be removed in next version. Please use `random_state` instead.\n",
      "  'Please use `random_state` instead.', LGBMDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit LGBMClassifier fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:282: LGBMDeprecationWarning: The `seed` parameter is deprecated and will be removed in next version. Please use `random_state` instead.\n",
      "  'Please use `random_state` instead.', LGBMDeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:282: LGBMDeprecationWarning: The `seed` parameter is deprecated and will be removed in next version. Please use `random_state` instead.\n",
      "  'Please use `random_state` instead.', LGBMDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit LGBMClassifier fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:282: LGBMDeprecationWarning: The `seed` parameter is deprecated and will be removed in next version. Please use `random_state` instead.\n",
      "  'Please use `random_state` instead.', LGBMDeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:282: LGBMDeprecationWarning: The `seed` parameter is deprecated and will be removed in next version. Please use `random_state` instead.\n",
      "  'Please use `random_state` instead.', LGBMDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit LGBMClassifier fold 4\n",
      "Fit XGBClassifier fold 1\n",
      "Fit XGBClassifier fold 2\n",
      "Fit XGBClassifier fold 3\n",
      "Fit XGBClassifier fold 4\n",
      "Stacker score: 0.64352\n"
     ]
    }
   ],
   "source": [
    "# Using stacking for the models\n",
    "# https://www.kaggle.com/yekenot/simple-stacker-lb-0-284/code (Version 8)\n",
    "stack = Ensemble(n_splits=4,\n",
    "        stacker = log_model,\n",
    "        base_models = (lgb_model, lgb_model2, lgb_model3, xgb_model))        \n",
    "\n",
    "test = test.drop(['id'],axis=1)\n",
    "y_pred = stack.fit_predict(X, y, test)\n",
    "sub['target'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.029547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.028325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    target\n",
       "0   0  0.029547\n",
       "1   1  0.028325"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.to_csv('submission_7.csv', index=False, float_format='%.5f') \n",
    "gc.collect()\n",
    "sub.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8.928160e+05</td>\n",
       "      <td>892816.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.441535e+05</td>\n",
       "      <td>0.036393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.296830e+05</td>\n",
       "      <td>0.021028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.020521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.720218e+05</td>\n",
       "      <td>0.027141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.443070e+05</td>\n",
       "      <td>0.031319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.116308e+06</td>\n",
       "      <td>0.038425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.488026e+06</td>\n",
       "      <td>0.990077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id         target\n",
       "count  8.928160e+05  892816.000000\n",
       "mean   7.441535e+05       0.036393\n",
       "std    4.296830e+05       0.021028\n",
       "min    0.000000e+00       0.020521\n",
       "25%    3.720218e+05       0.027141\n",
       "50%    7.443070e+05       0.031319\n",
       "75%    1.116308e+06       0.038425\n",
       "max    1.488026e+06       0.990077"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
