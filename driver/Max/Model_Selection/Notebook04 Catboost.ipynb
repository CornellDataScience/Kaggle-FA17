{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "45ba73d4-6c4c-40bb-9390-7dfc956c555d",
    "_uuid": "dbd332f83c89108c4e641218f5c4e7b9cd325b80",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters used in the kernel https://www.kaggle.com/aharless/simple-catboost-cv-lb-281?scriptVersionId=1653299\n",
    "MAX_ROUNDS = 650\n",
    "OPTIMIZE_ROUNDS = False\n",
    "LEARNING_RATE = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7e199c98-16b0-45e7-a6d1-bdd9325c2631",
    "_uuid": "b277fe426336d65ac71f0e6ac96c7ee16d02074c"
   },
   "source": [
    "Notes from original user in Kaggle:<br>\n",
    "I recommend initially setting <code>MAX_ROUNDS</code> fairly high and using <code>OPTIMIZE_ROUNDS</code> to get an idea of the appropriate number of rounds (which, in my judgment, should be close to the maximum value of the optimized <code>tree_count</code> among all folds, maybe even a bit higher if your model is adequately regularized...or maybe not:  it's also be a good idea to set verbose on sometimes and look at the pattern of validation scores for each fold).  Then I would turn off <code>OPTIMIZE_ROUNDS</code> and set <code>MAX_ROUNDS</code> to the appropraite number of total rounds.  The problem with \"early stopping\" by choosing the best round for each fold is that it overfits to the validation set (which, overall, is the same as the training set when using k-fold).    It's therefore liable not to produce the optimal model for predicting test data, and if it's used to produce validation data for stacking/ensembling with other models, it would cause this one to have too much weight in the ensemble.\n",
    "\n",
    "Also, CatBoost is notoriously slow.  If you want to run it in Kaggle's environment, you need to set a high learning rate (as I have done here, though I'm starting to push it a little now while Kaggle seems to be running fast) so that it will get to a decent fit reasonably quickly.  If you want to use it to do well in this competition, you need to set a lower learning rate and run it for a long time (higher <code>MAX_ROUNDS</code>).  That means you either need to run it overnight or run it on a fast computer with multiple cores (or both)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook05 Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timeline: 2017/11/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I. Import Packages, define functions and import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b7258128-55f9-4543-8611-5e0a6661837b",
    "_uuid": "72171ee53e170096d37a18eef84682fa348ae5c4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "3d16f16e-12cc-4b41-b7bd-fa05ce44770c",
    "_uuid": "154b078a7e86c0a5a328118a61d28e2581bb3b0a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute gini\n",
    "# from CPMP's kernel https://www.kaggle.com/cpmpml/extremely-fast-gini-computation\n",
    "@jit\n",
    "def eval_gini(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    ntrue = 0\n",
    "    gini = 0\n",
    "    delta = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n-1, -1, -1):\n",
    "        y_i = y_true[i]\n",
    "        ntrue += y_i\n",
    "        gini += y_i * delta\n",
    "        delta += 1 - y_i\n",
    "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "52b50086-b405-4598-b11c-97887cdcce8e",
    "_uuid": "07a5a5782894611e9006ae1b399b0b8fb8a0f06b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import files\n",
    "train_df = pd.read_csv('/Users/maxji/Desktop/Kaggle/0SafeDriver/data/train.csv', na_values=\"-1\") # .iloc[0:200,:]\n",
    "test_df = pd.read_csv('/Users/maxji/Desktop/Kaggle/0SafeDriver/data/test.csv', na_values=\"-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II. Data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "1b36eb15-ee01-43a3-8766-27650f98158d",
    "_uuid": "6255e3c12616b0279cef5c1bdec97751bb72d8b8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Process data\n",
    "id_test = test_df['id'].values\n",
    "id_train = train_df['id'].values\n",
    "\n",
    "train_df = train_df.fillna(999)\n",
    "test_df = test_df.fillna(999)\n",
    "\n",
    "col_to_drop = train_df.columns[train_df.columns.str.startswith('ps_calc_')]\n",
    "train_df = train_df.drop(col_to_drop, axis=1)  \n",
    "test_df = test_df.drop(col_to_drop, axis=1)  \n",
    "\n",
    "for c in train_df.select_dtypes(include=['float64']).columns:\n",
    "    train_df[c]=train_df[c].astype(np.float32)\n",
    "    test_df[c]=test_df[c].astype(np.float32)\n",
    "for c in train_df.select_dtypes(include=['int64']).columns[2:]:\n",
    "    train_df[c]=train_df[c].astype(np.int8)\n",
    "    test_df[c]=test_df[c].astype(np.int8)\n",
    "    \n",
    "y = train_df['target']\n",
    "X = train_df.drop(['target', 'id'], axis=1)\n",
    "y_valid_pred = 0*y\n",
    "X_test = test_df.drop(['id'], axis=1)\n",
    "y_test_pred = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "III. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "7c6e4823-4e8c-4408-b961-576d469e9241",
    "_uuid": "6aa7ada2193c2e4b8a63eebda925cee5023b45b0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up folds\n",
    "K = 5\n",
    "kf = KFold(n_splits = K, random_state = 1, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "5d8108f3-e9e8-45d6-93b5-740eb7b4b10b",
    "_uuid": "581c3f15f294378a0e2ac3305e9e3d375f664b21",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up classifier\n",
    "# Original parameters from the kernel\n",
    "\"\"\"model = CatBoostClassifier(\n",
    "    learning_rate=LEARNING_RATE, \n",
    "    depth=6, \n",
    "    l2_leaf_reg = 14, \n",
    "    iterations = MAX_ROUNDS,\n",
    "#    verbose = True,\n",
    "    loss_function='Logloss'\n",
    ")\"\"\"\n",
    "\n",
    "# New parameters for my kernel\n",
    "cat_params = {}\n",
    "cat_params['iterations'] = 900\n",
    "cat_params['depth'] = 8\n",
    "cat_params['rsm'] = 0.95\n",
    "cat_params['learning_rate'] = 0.03\n",
    "cat_params['l2_leaf_reg'] = 3.5  \n",
    "cat_params['border_count'] = 8\n",
    "cat_params['gradient_iterations'] = 4\n",
    "model = CatBoostClassifier(**cat_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "c4e48347-920f-4ba7-8b37-cfbaab4c3c00",
    "_uuid": "2b9ed96c98b705d3e4bf2a3d60323dfab4332674",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold  0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-69f8d64baf40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m\"  N trees = \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_count_\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mfit_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m# Generate validation predictions for this fold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, cat_features, sample_weight, baseline, use_best_model, eval_set, verbose, plot)\u001b[0m\n\u001b[0;32m   1082\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mCatBoost\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m         \"\"\"\n\u001b[1;32m-> 1084\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcat_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_best_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1085\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1086\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_classes\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, cat_features, sample_weight, baseline, use_best_model, eval_set, verbose, plot)\u001b[0m\n\u001b[0;32m    443\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mlog_fixup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    446\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcalc_feature_importance\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_feature_importance\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_importance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoostBase._train (c:\\users\\donskov\\.ya\\build\\build_root\\9tj5\\0001f4\\catboost\\python-package\\catboost\\_catboost.pyx.cpp:15616)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train (c:\\users\\donskov\\.ya\\build\\build_root\\9tj5\\0001f4\\catboost\\python-package\\catboost\\_catboost.pyx.cpp:10531)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train (c:\\users\\donskov\\.ya\\build\\build_root\\9tj5\\0001f4\\catboost\\python-package\\catboost\\_catboost.pyx.cpp:10329)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run Training and CV\n",
    "for i, (train_index, test_index) in enumerate(kf.split(train_df)):\n",
    "    \n",
    "    # Create data for this fold\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "    X_train, X_valid = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "    print( \"\\nFold \", i)\n",
    "    \n",
    "    # Run model for this fold\n",
    "    if OPTIMIZE_ROUNDS:\n",
    "        fit_model = model.fit( X_train, y_train, \n",
    "                               eval_set=[X_valid, y_valid],\n",
    "                               use_best_model=True\n",
    "                             )\n",
    "        print( \"  N trees = \", model.tree_count_ )\n",
    "    else:\n",
    "        fit_model = model.fit( X_train, y_train )\n",
    "        \n",
    "    # Generate validation predictions for this fold\n",
    "    pred = fit_model.predict_proba(X_valid)[:,1]\n",
    "    print( \"  Gini = \", eval_gini(y_valid, pred) )\n",
    "    y_valid_pred.iloc[test_index] = pred\n",
    "    \n",
    "    # Accumulate test set predictions\n",
    "    y_test_pred += fit_model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "y_test_pred /= K  # Average test set predictions\n",
    "\n",
    "print( \"\\nGini for full training set:\" )\n",
    "eval_gini(y, y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0e3dfd76-c566-4b8d-a460-b56e964d0772",
    "_uuid": "e61bf4e22c1c29c8358caeecb6e67d6658f2005d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save validation predictions for stacking/ensembling\n",
    "val = pd.DataFrame()\n",
    "val['id'] = id_train\n",
    "val['target'] = y_valid_pred.values\n",
    "val.to_csv('cat_valid.csv', float_format='%.6f', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f4cbef2c-e52b-4afb-b8ef-904ee9b5f9d5",
    "_uuid": "380fc8053d00cd8bb2796bfd2b59d10cbc4ce7e1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "sub = pd.DataFrame()\n",
    "sub['id'] = id_test\n",
    "sub['target'] = y_test_pred\n",
    "sub.to_csv('cat_submit.csv', float_format='%.6f', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "10f560d4-ca82-480d-a923-642bedcb3822",
    "_uuid": "f72a82b52898ce7478777be8ad42e2c91b238a77"
   },
   "source": [
    "Notes from original kernel: <br>\n",
    "CV scores certainly seem to be correlated with public LB scores, but the correlation is not nearly as strong as one might hope. The difference so far ranges from .0019 to .0032 for this script without <code>OPTIMIZE_ROUNDS</code> and can be higher when <code>OPTIMIZE_ROUNDS</code> is set (which is to be expected, since that parameter causes the fit to select on each fold for the best CV performance without generally achieving a comparable improvement in LB performance).  Versions 17 and 18 (substantively identical) have slightly better public LB performance than earlier versions, when I sort my submissions by score.  (If one wants to be optimistic about the less significant digits of the LB score, maybe a difference of .0019 is really .0024, and a difference of .0032 is really .0027, so maybe the CV is working better than it first appears.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "43b577f2759c49e871fc47ef9c888fae9b1bfb50",
    "collapsed": true
   },
   "source": [
    "Insight: <br>\n",
    "Catboost model has a score of 0.281 similar to that in xgboost model. The distribution of the model is different from the xgboost model, meaning that stacking work well with those two models. However, it takes more time to train and predict.\n",
    "<br>\n",
    "From this experiment I'm thinking of focusing more on boosting models because they perform way better than other models. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
